pd.read_csv('./data/anscombe.csv')
quit
install.packages(c("ada", "adabag", "C50", "Epi"))
# neural network
library(neuralnet)
net.infert <- neuralnet(case ~ age+parity+induced+spontaneous, data=trainData, hidden=3,
err.fct='ce', linear.output=F, likelihood=T)
library(nnet)
library(rpart)
head(trainData)
# Holdout
data(iris)
nrow(iris)
set.seed(42)
idx <- sample(2, nrow(iris), replace=T, prob=c(.7, .3))
trainData <- iris[idx==1, ]
testData <- iris[idx==2, ]
nrow(trainData)
nrow(testData)
# cross validation
data(iris)
nrow(iris)
set.seed(42)
k=10
iris <- iris[sample(nrow(iris)), ] # Randomly shuffle the data
folds <- cut(seq(1, nrow(iris)), breaks=k, labels=F)
trainData = list(0)
testData = list(0)
for (i in 1:k) { # Perform 10 fold cross validation
testIdx <- which(folds==i, arr.ind=T)
testData[[i]] <- iris[testIdx, ]
trainData[[i]] <- iris[-testIdx, ]
}
head(trainData[[1]]) # 첫 번째 fold
head(testData[[2]]) # 두 번째 fold
data(iris)
folds
trainData = list(0)
testData = list(0)
for (i in 1:k) { # Perform 10 fold cross validation
testIdx <- which(folds==i, arr.ind=T)
testData[[i]] <- iris[testIdx, ]
trainData[[i]] <- iris[-testIdx, ]
}
head(trainData[[1]]) # 첫 번째 fold
head(testData[[2]]) # 두 번째 fold
data(iris)
iris <- subset(iris, Species == 'setosa' | Species == 'versicolor')
iris$Species <- factor(iris$Species)
str(iris)
set.seed(42)
iris <- iris[sample(nrow(iris)), ] # Ramdomly shuffle the data
# 이미 shuffled 되어 있으므로...
trainData <- iris[1:nrow(iris)*.7, ]
dim(trainData)
head(trainData)
testData <- iris[((nrow(iris)*.7) + 1) : nrow(iris), ]
dim(testData)
nrow(trainData)
nrow(testData)
library(nnet)
library(rpart)
head(trainData)
nn.iris <- nnet(Species ~., data=trainData, size=2, rang=0, decay=5e-4, maxit=200)
dt.iris <- rpart(Species ~., data=trainData)
nn_pred <- predict(nn.iris, testData, type='class')
nn_pred
str(nn_pred)
dt_pred <- predict(dt.iris, testData, type='class')
# install.packages('e1071')
library(e1071)
library(caret)
nn_pred <- as.factor(nn_pred)
str(testData$Species)
nn_con = confusionMatrix(nn_pred, testData$Species)
nn_con$table
dt_con = confusionMatrix(dt_pred, testData$Species)
dt_con
dt_con$table
nn_con$overall['Accuracy']
dt_con$overall['Accuracy']
nn_con$byClass['Sensitivity']
dt_con$byClass['Sensitivity']
accuracy <- c(nn_con$overall['Accuracy'], dt_con$overall['Accuracy'])
precision <- c(nn_con$byClass['Pos Pred Value'], dt_con$byClass['Pos Pred Value'])
recall <- c(nn_con$byClass['Sensitivity'], dt_con$byClass['Sensitivity'])
f1 <- 2*(precision*recall) / (precision + recall)
result <- data.frame(rbind(accuracy, precision, recall, f1))
names(result) <- c('nnet', 'decision tree')
result
data(infert)
head(infert)
# parity : the number of times a female has given birth
# case :case status (1 : case, 0 :controlled)
# stratum : ??
# pooled.stratum : ??
tail(infert)
# ROC graph
set.seed(42)
infert <- infert[sample(nrow(infert)), ] # suffling
infert <- infert[, c('age', 'parity', 'induced', 'spontaneous', 'case')]
head(infert)
nrow(infert)
trainData <- infert[1:(nrow(infert)*.7), ]
testData <- infert[((nrow(infert)*.7 + 1) : nrow(infert)), ]
nrow(trainData)
nrow(testData)
# neural network
library(neuralnet)
net.infert <- neuralnet(case ~ age+parity+induced+spontaneous, data=trainData, hidden=3,
err.fct='ce', linear.output=F, likelihood=T)
n_test <- subset(testData, select=-case)
nn_pred <- neuralnet::compute(net.infert, n_test)
testData$net_pred <- nn_pred$net.result
head(testData)
# decison tree
# install.packages('C50')
library(C50)
trainData$case <- factor(trainData$case)
dt.infert <- C50::C5.0(case ~ age + parity + induced + spontaneous, data=trainData)
testData$dt_pred <- predict(dt.infert, testData, type='prob')[, 2]
head(testData)
# install.packages('Epi')
library(Epi)
neural_ROC <- ROC(form=case ~ net_pred, data = testData, plot='ROC')
dtree_ROC <- ROC(form=case ~ dt_pred, data = testData, plot='ROC')
install.packages('cmprsk')
library(cmprsk)
neural_ROC <- ROC(form=case ~ net_pred, data = testData, plot='ROC')
# install.packages('Epi')
library(Epi)
install.packages('Epi')
# install.packages('Epi')
library(Epi)
neural_ROC <- ROC(form=case ~ net_pred, data = testData, plot='ROC')
dtree_ROC <- ROC(form=case ~ dt_pred, data = testData, plot='ROC')
head(testData)
# net_pred가 높은 순으로 정렬
gain_tbl <- testData[order(testData$net_pred, decreasing = T), ][, c('case', 'net_pred')]
head(gain_tbl, 10)
str(gain_tbl)
gain_tbl$net_pred <- as.numeric(gain_tbl$net_pred)
gain_tbl %>%
select(case, net_pred) %>%
mutate(group = cut(net_pred,
breaks = seq(0, 1, .1),
include.lowest = T, # 0을 그룹에 포함시키기 위해 반드시 필요, 아니면 NA값 반환됨.
labels=c('0-10', '10-20', '20-30', '30-40', '40-50',
'50-60', '60-70', '70-80', '80-90', '90-100')))
nrow(testData) # 74
nrow(subset(testData, testData$case == 1)) # 28
quanted <- quantile(gain_tbl$net_pred, seq(0,1,.1))
str(quanted)
quanted['90%']
unname(quanted['90%'])
transformed <- transform(gain_tbl,
group=cut(net_pred, breaks = quanted, include.lowest = T,
labels=c('0-10', '10-20', '20-30', '30-40', '40-50',
'50-60', '60-70', '70-80', '80-90', '90-100')))
str(transformed)
transformed %>% group_by(group) %>% summarise(sum.case=sum(case)) %>% arrange(desc(group))
# base lift
28/74
par(new=T)
plot(d_p, col='blue')
abline(a=0, b=1)
n_lift <- performance(n_r, 'lift', 'rpp')
plot(n_lift, col='red')
abline(v=.2)
# install.packages('ROCR')
library(ROCR)
str(testData)
testData$net_pred <- as.numeric(testData$net_pred )
n_r <- prediction(testData$net_pred, testData$case)
d_r <- prediction(testData$dt_pred, testData$case)
n_p <- performance(n_r, 'tpr', 'fpr')
d_p <- performance(d_r, 'tpr', 'fpr')
plot(n_p, col='red')
par(new=T)
plot(d_p, col='blue')
abline(a=0, b=1)
n_lift <- performance(n_r, 'lift', 'rpp')
plot(n_lift, col='red')
abline(v=.2)
data(infert)
head(infert)
# parity : the number of times a female has given birth
# case :case status (1 : case, 0 :controlled)
# stratum : ??
# pooled.stratum : ??
tail(infert)
# neural network
library(neuralnet)
net.infert <- neuralnet(case ~ age+parity+induced+spontaneous, data=trainData, hidden=3,
err.fct='ce', linear.output=F, likelihood=T)
net.infert
names(net.infert)
net.infert$result.matrix
out <- cbind(net.infert$covariate, net.infert$net.result[[1]])
out
dimnames(out) <- list(NULL, c("age", "party", "induced", "spontaneous", "nn-output"))
out
dimnames(out) <- list(NULL, c("age", "party", "induced", "spontaneous", "nn-output"))
dimnames(out) <-  c("age", "party", "induced", "spontaneous", "nn-output")
out %>% dim
dimnames(out) <- list(NULL, c("age", "parity", "induced", "spontaneous", "nn-output_1", "nn-output_2"))
n_test <- subset(testData, select=-case)
head(out)
colnames(infert)
infert %>% select(-c('education', 'stratum', 'pooled.stratum'))-> infert_cont
infert_cont$case
source('~/ADP_guide_book/ADP_guide_book.R', encoding = 'UTF-8', echo=TRUE)
install.packages(c("clusterGeneration", "forecast"))
setwd("~/ADP_guide_book")
source('~/ADP_guide_book/ADP_guide_book.R', encoding = 'UTF-8', echo=TRUE)
source('~/ADP_guide_book/ADP_guide_book.R', encoding = 'UTF-8', echo=TRUE)
source('~/ADP_guide_book/ADP_guide_book.R', encoding = 'UTF-8', echo=TRUE)
source('~/ADP_guide_book/ADP_guide_book.R', encoding = 'UTF-8', echo=TRUE)
d %>% group_by(year) %>% summarise(mean.count=mean(count))
d %>% group_by(year) %>% summarise(sum.count=sum(count))
iris %>% group_by(Species) %>% summarise(mean.Sepal.Length=mean(Sepal.Length))
mtcars %>% group_by(cyl, gear) %>% summarise(newvar = sum(wt))
# ddply(d, 'year', summarise, mean.count=mean(count))
d %>% group_by(year) %>%  summarise(mean.count = mean(count))
# ddply(d, 'year', transform, total.count = sum(count))
d %>% group_by(year) %>% transform(total.count = sum(count))
# data.table
# install.packages('data.table')
library(data.table)
DT = data.table(x=c('b', 'b', 'b', 'a', 'a'), v=rnorm(5))
DT
data(cars)
head(cars)
CARS <- data.table(cars)
head(CARS)
tables()
sapply(CARS, class)
DT
DT[2,]
DT[DT$x=='b', ]
DT[x=="b", ]
runif(10)
grpsize <- ceiling(1e7/26^2)
grpsize
grpsize*26
grpsize*26*26
tt <- system.time(DF <- data.frame(
x=rep(LETTERS, each=26*grpsize),
y=rep(letters, each=grpsize),
v=runif(grpsize*26^2),
stringsAsFactors = F
))
tt
head(DF, 3)
tail(DF, 3)
dim(DF)
tt <- system.time(ans1 <- DF[DF$x == 'R' & DF$y == 'h', ])
tt
head(ans1)
head(DF,100)
DT <- data.table(DF)
setkey(DT,x,y)
ss <- system.time(ans2 <- DT[J('R', 'h')]) # binary search
ss
# bad case for using data.table, # 1.425 secs
system.time(ans2 <- DF[DF$x == 'R' & DF$y == 'h', ])
mapply(identical, ans1, ans2)
DT[, sum(v)]
DT[, sum(v), by=x]
ttt <- system.time(tt <- tapply(DT$v, DT$x, sum))
ttt
sss <- system.time(ss <- DT[, sum(v), by=x])
sss <- system.time(ss <- DT[, sum(v), by='x,y'])
sss
data(iris)
head(iris)
str(iris)
summary(iris)
cov(iris[, 1:4])
cor(iris[, 1:4])
# NA
y <- c(1,2,3,NA)
is.na(y)
head(mydat)
x <- c(1,2,NA,3)
mean(x)
mean(x, na.rm=T)
# install.packages('Amelia')
library(Amelia)
data("freetrade")
head(freetrade)
dim(freetrade)
str(freetrade)
a.out <- amelia(freetrade, m=5, ts='year', cs='country')
hist(a.out$imputations[[3]]$tariff, col='gray', border='white')
save(a.out, file = 'imputation.RData')
write.amelia(obj=a.out, file.stem = 'outdata')
missmap(a.out)
freetrade$tariff <- a.out$imputations[[5]]$tariff
missmap(freetrade)
# ouliers
x = rnorm(100)
boxplot(x)
x <- c(x, 19, 28, 30)
outwidth = boxplot(x)
outwidth$out
# install.packages('outliers')
library(outliers)
set.seed(1234)
y = rnorm(100)
outlier(y)
outlier(y, opposite = T)
a=c(10,20,30)
b=c(40,30,20)
a
t(b)
x=a%*%t(b)
x
y=a*b
y
head(iris)
library(MASS)
data(Animals)
head(Animals)
# regression
set.seed(2)
x = runif(10,0,11)
y = 2 + 3*x + rnorm(10,0,.2)
dfrm = data.frame(x,y)
dfrm
lm(y~x, data=dfrm)
set.seed(2)
u = runif(10,0,11)
v = runif(10,11,20)
w = runif(10,1,30)
y = 3 + .1*u + 2*v -3*w + rnorm(10,0,.1)
dfrm = data.frame(y, u, v, w)
dfrm
m <- lm(y~u+v+w)
m
summary(m)
library(MASS)
head(ChickWeight)
summary(ChickWeight)
dim(ChickWeight)
ChickWeight$Chick
ChickWeight %>% filter(Chick == 5)
length(ChickWeight$Chick)
head(ChickWeight)
str(ChickWeight)
ChickWeight %>% filter((Chick == 1) & (Diet == 1)) -> Chick
lm(weight ~ Time, Chick) -> lm_chick
summary(lm_chick)
head(cars)
cars$speed2 <- cars$speed^2
head(cars)
cars[, c(3,1,2)] %>% head()
lm(dist ~ speed + speed2, data=cars)
summary(lm(dist ~ speed + speed2, data=cars))
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9)
y <- c(5, 3, 2, 3, 4, 6, 10, 12 ,18)
df1 <-data.frame(x, y)
df1
plot(df1)
library(ggplot2)
ggplot(data=df1) + geom_point(aes(df1$x, df1$y))
ggplot(data=df1, aes(x, y)) + geom_point(color='blue', alpha=.5, size=3)
df1$x2 <- x^2
df1
lm(y~x, data=df1)
summary(lm(y~x, data=df1))
plot(lm(y~x, data=df1))
plot(lm(y~x+x2, data=df1))
x3 <- c(6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8)
x4 <- c(60, 52, 20, 47, 33, 22, 6, 44, 22, 26, 34, 12, 12)
Y <- c(78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7, 72.5, 93.1, 115.9, 83.9, 113.3, 109.4)
df <- data.frame(x1, x2, x3, x4, Y)
head(df)
write.csv(df, './data/df_p390.csv')
a <- lm(Y ~ x1 + x2 + x3 + x4, data = df)
a
summary(a)
b <- lm(Y ~ x1 + x2 + x4, data=df)
b
summary(b)
c <- lm(Y ~x1+x2, data=df)
c
summary(c)
step(lm(Y ~ 1, df), scope=list(lower=~1, upper=~x1+x2+x3+x4), direction='forward')
step(lm(Y ~ 1, df), scope=list(lower=~1, upper=~x1+x2+x3+x4), direction='both')
data(hills)
head(hills)
step(lm(time ~ 1, hills), scope=list(lower=~1, upper=~dist+climb), direction='forward')
age <- c(7, 7, 8, 8, 8, 9, 11, 12, 12, 13, 13, 14, 14, 15, 16, 17, 17, 17, 17, 19, 19, 20, 23, 23, 23)
length(age)
height <- c(109, 112, 124, 125, 127, 130, 139, 150, 146, 155, 156, 153, 160, 158, 160, 153, 174, 176, 171, 156,
174, 178, 180, 175, 179)
length(height)
weight <- c(13.1, 12.9, 14.1, 16.2, 21.5, 17.5, 30.7, 28.4, 25.1, 31.5, 39.9, 42.1, 45.6, 51.2, 35.9,
34.8, 44.7, 60.1, 42.6, 37.2, 54.6, 64, 73.8, 51.1, 71.5)
length(weight)
bmp <- c(68, 65, 64, 67, 93, 68, 89, 69, 67, 68, 89, 90, 93, 93, 66, 70, 70, 92, 69, 72, 86, 86, 97, 71, 95)
length(bmp)
fev <- c(32, 19, 22, 41, 52, 44, 28, 18, 24, 23, 39, 26, 45, 45, 31, 29, 49, 29, 38, 21, 37, 34, 57, 33, 52)
length(fev)
rv <- c(258, 449, 441, 234, 202, 308, 305, 369, 312, 413, 206, 253, 174, 158, 302, 204, 187, 188, 172, 216,
184, 225, 171, 224, 225)
length(rv)
frc <- c(183, 245, 268, 146, 131, 155, 179, 198, 194, 225, 142, 191, 139, 124, 133, 118, 104, 129, 130, 119, 118, 148, 108, 131, 127)
length(frc)
tlc <- c(137, 134, 147, 124, 104, 118, 119, 103, 128, 136, 95, 121, 108, 90, 101, 120, 103, 130, 103,
81, 101, 135, 98, 113, 101)
length(tlc)
pemax <- c(95, 85, 100, 85, 95, 80, 65, 110, 70, 95, 110, 90, 100, 80, 134, 134, 165, 120, 130, 85, 85, 160, 165, 95, 195)
length(pemax)
bio <- data.frame(age, weight, bmp, fev, rv, frc, tlc, pemax)
write.csv(bio, './data/bio.csv', row.names = F)
bio <- read.csv('./data/bio.csv')
head(bio)
step(lm(pemax~1, bio), scope=list(lower=~1, upper=~age+height+weight+bmp+rv+frc+tlc), direction = 'forward')
step(lm(pemax~age+height+weight+bmp+rv+frc+tlc, bio), direction = 'backward')
step(lm(pemax~1, bio), scope=list(lower=~1, upper=~age+height+weight+bmp+rv+frc+tlc), direction = 'both')
# install.packages('Hmisc')
library(Hmisc)
data(mtcars)
head(mtcars)
dim(mtcars)
drat <- mtcars$drat
disp <- mtcars$disp
plot(drat, disp)
cor(drat, disp)
rcorr(as.matrix(mtcars), type='pearson')
cov(mtcars)
rcorr(as.matrix(mtcars), type='spearman')
korean <- c(85, 75, 65, 78, 59, 60, 90, 100, 99, 91, 70)
math <- c(80, 60, 75, 40, 50, 64, 70, 78, 90, 98, 50)
english <- c(80, 70, 69, 79, 80, 95, 98, 97, 67, 80, 59)
science <- c(90, 100, 50, 80, 67, 89, 60, 79, 89, 80, 100)
test <- data.frame(korean, math, english, science)
test
rcorr(as.matrix(test), type='spearman')
data(eurodist)
eurodist
loc <- cmdscale(eurodist)
x <- loc[,1]
y <- loc[,2]
plot(x, y, type='n', main='eurodist')
text(x, y, rownames(loc), cex=.8)
abline(v=0, h=0)
library(datasets)
data(USArrests)
head(USArrests)
summary(USArrests)
fit <- princomp(USArrests, cor=T)
mfrow(c(1, 1))
par = mfrow(c(1, 1))
par(mfrow(c(1, 1)))
par(mfrow=c(1,1))
source('~/ADP_guide_book/ADP_guide_book.R', encoding = 'UTF-8', echo=TRUE)
library(neuralnet)
data("infert")
head(infert)
str(infert)
summary(infert)
boxplot(infert)
library(tidyverse)
colnames(infert)
infert %>% select(-c('education', 'stratum', 'pooled.stratum'))-> infert_cont
infert_cont$case
par(mfrow=c(1,1))
boxplot(infert)
library(tidyverse)
colnames(infert)
infert %>% select(-c('education', 'stratum', 'pooled.stratum'))-> infert_cont
infert_cont$case
net.infert <- neuralnet(case ~ age + parity + induced + spontaneous, data=infert, hidden=2,
err.fct='ce', linear.output = F, likelihood = T)
net.infert
library(tidyverse)
colnames(infert)
infert %>% select(-c('education', 'stratum', 'pooled.stratum'))-> infert_cont
infert_cont$case
infert %>% select(-c('education', 'stratum', 'pooled.stratum'))-> infert_cont
infert %>% select(-education, -stratum, -pooled.stratum))-> infert_cont
infert %>% select(-education, -stratum, -pooled.stratum)-> infert_cont
infert
infert %>%
select(age, parity, induced, case, spontaneous)
library(tidyverse)
infert %>%
select(age, parity, induced, case, spontaneous)
source('~/ADP_guide_book/ADP_guide_book_1.R', encoding = 'UTF-8', echo=TRUE)
head(cars)
cars$speed2 <- cars$speed^2
cars$speed2 <- (cars$speed)^2
cars %>% head()
head(car)
mtcars
library(neuralnet)
data("infert")
head(infert)
str(infert)
summary(infert)
par(mfrow=c(1,1))
boxplot(infert)
library(tidyverse)
colnames(infert)
infert %>% select(-education, -stratum, -pooled.stratum)-> infert_cont
infert %>%
select(age, parity, induced, case, spontaneous)
infert %>%
select(age)
